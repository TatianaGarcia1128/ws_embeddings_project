{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytics & Data Science\n",
    "\n",
    "Universidad de Antioquia - ML2\n",
    "\n",
    "Feb 2024\n",
    "\n",
    "Natalia López Grisale CC. 1040048893\n",
    "Tatiana García Zuluaga CC. 1017198484\n",
    "Melissa Ortega Alzate CC.1036964792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/melissa/github/virtualenvs/udea/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Calculate distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Open Source library to build embeddings\n",
    "#! pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to read the text file\n",
    "def read_text_from_file(file_name):\n",
    "    \"\"\"\n",
    "    Read text data from a file.\n",
    "    Args:\n",
    "        file_name (str): The name of the file to read.\n",
    "    Returns:\n",
    "        str: The text read from the file.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split the text into a number of desired sentences\n",
    "def split_text(text, sentences_per_fragment):\n",
    "    \"\"\"\n",
    "    Split text into fragments based on a specified number of lines per fragment.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be split into fragments.\n",
    "        lines_per_fragment (int): The number of lines per fragment.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of fragments, where each fragment contains the specified number of lines.\n",
    "    \"\"\"\n",
    "    lines = text.split('.')\n",
    "    fragments = []\n",
    "    \n",
    "    for i in range(0, len(lines), sentences_per_fragment):\n",
    "        fragment = '.'.join(lines[i: i+sentences_per_fragment])\n",
    "        fragments.append(fragment)\n",
    "    \n",
    "    return fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. In your own words, describe what vector embeddings are and what they are useful for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are a numerical representation not only of language but also of its context and semantics. \n",
    "\n",
    "They are fixed-dimensional vectors constructed from texts so that ML models can understand audio, text, image, video instructions, etc. Embeddings are, therefore, a representation in a large dimensional space with the best possible meaning of context and semantics. Since embeddings are vectors, they can be manipulated with all traditional linear algebra techniques.\n",
    "\n",
    "These embeddings are useful in many areas, and thanks to them, recommendation systems (YouTube, Netflix), semantic search engines (YouTube), translators, ChatGPT, text classifiers, and in general, all language understanding AI models have been built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What do you think is the best distance criterion to estimate how far two embeddings ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "cat = np.array([0.9 , 0.8, 0.2])\n",
    "dog = np.array([0.8 , 0.85, 0.15])\n",
    "computer = np.array([0.1 , 0.3, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of working with vectors is that we can measure the distance between them, allowing us to assess how far apart one word is from another. Therefore, the similarity of two vectors can be measured as follows:\n",
    "\n",
    "- **Cosine distance:** This criterion takes into account the direction of the vectors by measuring the angle formed between them. When two vectors have the same direction, the angle between them is smaller. For example, two orthogonal vectors could represent words that are not very similar. Two vectors forming an angle of 180 degrees would represent two words with opposite or very different meanings. The smaller the angle, the more similar the vectors. This is a very efficient distance criterion for measuring the similarity between two words. \n",
    "\n",
    "The result is a number between 0 and 1. And as the next example shows, when the results is near to 1, the vectors have more similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance between cat and dog: 0.9954467122628464\n",
      "Cosine distance between cat and dog: 0.4379820840197298\n"
     ]
    }
   ],
   "source": [
    "# Cosine distance example using numpy\n",
    "\n",
    "# Calculate cat-dog\n",
    "dot_product = np.dot(cat, dog)\n",
    "norm_cat = np.linalg.norm(cat)\n",
    "norm_dog = np.linalg.norm(dog)\n",
    "print(f\"Cosine distance between cat and dog: {dot_product/(norm_cat * norm_dog)}\")\n",
    "\n",
    "# Calculate cat-dog\n",
    "dot_product = np.dot(cat, computer)\n",
    "norm_cat = np.linalg.norm(cat)\n",
    "norm_computer = np.linalg.norm(computer)\n",
    "print(f\"Cosine distance between cat and dog: {dot_product/(norm_cat * norm_computer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Euclidean distance:** this criterio calculates the distance between the vectors taking into account their magnitud. This distance is calculated as the difference between the vectors. The result is a number between 0 and infinite.  The smaller the distance, the larger the similarity between the vectors. In the following example, the euclidean distance bewtween cat and computer is higher than the one between cat and dog indicating cat and computer are more further apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between cat and dog: 0.12247448713915887\n",
      "Euclidean distance between cat and computer: 1.174734012447073\n"
     ]
    }
   ],
   "source": [
    "# Euclidean distance example\n",
    "print(f\"Euclidean distance between cat and dog: {np.linalg.norm(cat - dog)}\")\n",
    "print(f\"Euclidean distance between cat and computer: {np.linalg.norm(cat - computer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dot product:** this metric take the not only the angel but the magnitud of the vectors.  This product will be negative when the vectors are opposite and positive if they have the same direction. Therefore, when the data is normalized, it is the same as calculate the cosine distance. In the example, the metric is larger for cat-dog distance, indicating that they have higher similarity than cat-computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dot product distance between cat and dog: 1.4300000000000002\n",
      " Dot product distance between cat and computer: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Dot product distance example\n",
    "print(f\" Dot product distance between cat and dog: {np.dot(cat, dog)}\")\n",
    "print(f\" Dot product distance between cat and computer: {np.dot(cat, computer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a challenge to to calculate the distances between hunderds of vectors with hunderds of dimensions. Some distances metrics are more compute-heavy than others and it is important to balance the speed of compute and the accuracy. For sentence transformes **cosine distance** would be the best criteria for the calculation. Also the models onwoards use them and it is important to match the distance with the one the model is already using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descripción de la imagen](https://weaviate.io/assets/images/hero-183a22407b0eaf83e53d574aee0a049a.png)\n",
    "\n",
    "Figure taken from: https://weaviate.io/blog/distance-metrics-in-vector-search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let us build a Q&A (question answering) system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Pick a text\n",
    "\n",
    "The text was taken and edited from: https://aws.amazon.com/what-is/machine-learning/?nc1=h_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the text is 14044 characters\n",
      "\n",
      "The text up to character 342 is:\n",
      " Machine learning is the science of developing algorithms and statistical models that computer systems use to perform tasks without explicit instructions, relying on patterns and inference instead. Computer systems use machine learning algorithms to process large quantities of historical data and identify data patterns. This allows them to p\n"
     ]
    }
   ],
   "source": [
    "# Load the text using the predefined function\n",
    "text = read_text_from_file('suggested_text/sample_1.txt')\n",
    "\n",
    "# Text description\n",
    "print(\"The length of the text is\", len(text), \"characters\\n\")\n",
    "print(\"The text up to character 342 is:\\n\", text[:342])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split that text into meaningful chunks/pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment 1: Machine learning is the science of developing algorithms and statistical models that computer systems use to perform tasks without explicit instructions, relying on patterns and inference instead. Computer systems use machine learning algorithms to process large quantities of historical data and identify data patterns\n",
      "\n",
      "Fragment 2:  This allows them to predict outcomes more accurately from a given input data set. For example, data scientists could train a medical application to diagnose cancer from x-ray images by storing millions of scanned images and the corresponding diagnoses\n",
      "\n",
      "Fragment 3: \n",
      "Machine learning helps businesses by driving growth, unlocking new revenue streams, and solving challenging problems. Data is the critical driving force behind business decision-making but traditionally, companies have used data from various sources, like customer feedback, employees, and finance\n",
      "\n",
      "Fragment 4:  Machine learning research automates and optimizes this process. By using software that analyzes very large volumes of data at high speeds, businesses can achieve results faster\n",
      "\n",
      "Fragment 5: \n",
      "Letâs take a look at machine learning applications in some key industries:\n",
      "Machine learning can support predictive maintenance, quality control, and innovative research in the manufacturing sector. Machine learning technology also helps companies improve logistical solutions, including assets, supply chain, and inventory management\n",
      "\n",
      "Fragment 6:  For example, manufacturing giant 3M uses AWS Machine Learning to innovate sandpaper. Machine learning algorithms enable 3M researchers to analyze how slight changes in shape, size, and orientation improve abrasiveness and durability\n",
      "\n",
      "Fragment 7:  Those suggestions inform the manufacturing process.\n",
      "The proliferation of wearable sensors and devices has generated a significant volume of health data\n",
      "\n",
      "Fragment 8:  Machine learning programs can analyze this information and support doctors in real-time diagnosis and treatment. Machine learning researchers are developing solutions that detect cancerous tumors and diagnose eye diseases, significantly impacting human health outcomes\n",
      "\n",
      "Fragment 9:  For example, Cambia Health Solutions used AWS Machine Learning to support healthcare start-ups where they could automate and customize treatment for pregnant women.\n",
      "Financial machine learning projects improve risk analytics and regulation\n",
      "\n",
      "Fragment 10:  Machine learning technology can allow investors to identify new opportunities by analyzing stock market movements, evaluating hedge funds, or calibrating financial portfolios. In addition, it can help identify high-risk loan clients and mitigate signs of fraud\n",
      "\n",
      "\n",
      "The text was divided into 10 fragments\n"
     ]
    }
   ],
   "source": [
    "# Split the text into fragments\n",
    "fragments = split_text(text, 2)\n",
    "\n",
    "# Print the fragments\n",
    "for i, fragment in enumerate(fragments[:10]):\n",
    "    print(f\"Fragment {i+1}: {fragment}\\n\")\n",
    "\n",
    "# Print the total number of fragments\n",
    "print(f\"\\nThe text was divided into {i+1} fragments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Implement the embedding generation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 66\n",
      "\n",
      "Embedding for fragment 1:\n",
      " [-1.48474304e-02  7.25356862e-02  2.91582360e-03 -4.75783236e-02\n",
      " -7.07611069e-02 -4.08746339e-02 -2.42115725e-02  8.66363771e-05\n",
      " -8.34782571e-02  1.79940257e-02  9.69681889e-02  1.69683918e-01\n",
      " -7.51287714e-02  1.85720757e-01  5.01122735e-02 -2.02736065e-01\n",
      " -1.25303455e-02 -5.84237203e-02  4.42154631e-02  8.35931748e-02\n",
      " -3.54419723e-02 -7.57947415e-02  1.18775796e-02 -2.02463530e-02\n",
      " -3.27721760e-02 -9.49414372e-02 -4.07991782e-02 -1.10695690e-01\n",
      " -5.66377081e-02  1.07649215e-01 -5.01370691e-02 -1.66517437e-01\n",
      "  1.41108125e-01  8.77800509e-02  1.66838113e-02 -3.44206765e-02\n",
      " -8.37415364e-03  9.68867838e-02  7.52341002e-02 -1.49951339e-01\n",
      "  2.48079792e-01 -2.45287940e-02  7.27478713e-02 -4.47176807e-02\n",
      "  2.09419243e-02  2.75891215e-01  6.25376925e-02 -1.25490114e-01\n",
      " -2.54110415e-02  1.12973325e-01  7.29606440e-03  9.62402895e-02\n",
      " -1.86455518e-01  5.36349453e-02 -1.10063732e-01 -9.58444029e-02\n",
      " -6.10179566e-02  1.28090963e-01 -3.83325443e-02 -4.61676009e-02\n",
      " -1.60130914e-02  9.44783464e-02 -5.11696711e-02 -1.64657924e-02\n",
      "  6.75812140e-02  3.21506336e-02  2.19015956e-01 -4.59687337e-02\n",
      " -1.21099375e-01 -3.66299935e-02  1.69115830e-02 -7.11005041e-03\n",
      "  3.54999155e-02 -9.09921080e-02  1.85585637e-02  1.91143930e-01\n",
      " -9.74553525e-02  7.58259296e-02 -9.28267911e-02  5.06461561e-02\n",
      " -1.55176163e-01 -5.13103306e-02  1.27288058e-01 -5.00572957e-02\n",
      " -8.42435881e-02  7.91606233e-02 -9.79046002e-02  1.38845649e-02\n",
      " -1.32408654e-02 -3.80550250e-02  1.69347733e-01  1.20714344e-01\n",
      "  1.27871156e-01  2.18329970e-02 -1.33072332e-01 -9.52854156e-02\n",
      " -4.02549244e-02  1.54323831e-01  1.14825731e-02  9.42845643e-02\n",
      "  2.30691329e-01 -2.53462978e-02  1.21064961e-01  4.26260941e-02\n",
      "  3.71236876e-02 -5.46274409e-02 -2.29336828e-01 -6.93879053e-02\n",
      "  1.07605323e-01  2.19387468e-02 -6.01005405e-02 -6.17284104e-02\n",
      "  1.51230476e-03  8.31046104e-02  5.69016524e-02  1.03805801e-02\n",
      " -4.56558056e-02  1.67324170e-01  2.04275716e-02 -2.30014157e-02\n",
      " -2.91355908e-01 -1.06560417e-01  2.44514039e-03  5.80320619e-02\n",
      " -1.97920412e-01 -1.86419055e-01 -6.27571866e-02  5.30209467e-02\n",
      " -7.38889864e-03 -1.46145642e-01 -9.94343590e-03  1.45749666e-03\n",
      " -1.49202451e-01 -5.62001094e-02 -1.96991768e-02  2.67225713e-01\n",
      "  8.52464288e-02 -9.90089476e-02 -2.13918120e-01 -3.93398739e-02\n",
      "  7.35411495e-02 -8.09541568e-02  1.23989828e-01 -4.74078916e-02\n",
      "  6.05404787e-02  5.49084460e-03  1.65617708e-02  2.58919269e-01\n",
      " -3.72082926e-03 -2.19790824e-03 -3.37842643e-01  8.11567605e-02\n",
      "  2.61127576e-02 -9.85494163e-03 -3.44649591e-02 -5.18275760e-02\n",
      " -7.84630328e-02  1.54642865e-01 -1.54337669e-02  1.37152267e-03\n",
      " -1.38292179e-01  7.19408020e-02  1.43512245e-02  5.24193011e-02\n",
      "  6.06841780e-02 -6.09473288e-02 -2.21743271e-01  3.03786155e-02\n",
      " -2.29909837e-01  7.44130090e-02  4.82422821e-02  5.82607724e-02\n",
      " -6.78710192e-02  1.46223739e-01  1.42016247e-01  3.89629975e-02\n",
      " -2.24185642e-02  2.96991557e-01  1.04365714e-01  1.30979382e-02\n",
      " -9.71336514e-02  1.88383132e-01  5.58571555e-02  1.75899222e-01\n",
      " -3.11790407e-01 -2.86286995e-02  5.23113459e-02 -4.13246593e-03\n",
      " -7.84988105e-02 -1.15768462e-01  1.28576130e-01 -1.16873182e-01\n",
      " -9.18864533e-02 -4.50828411e-02  1.31720990e-01  9.44224447e-02\n",
      "  1.24063797e-03 -3.37296613e-02  5.97388335e-02 -6.40944093e-02\n",
      " -7.87482932e-02 -2.12908357e-01  3.71548645e-02  9.65103060e-02\n",
      "  5.23983873e-02 -2.49921232e-01  6.41113296e-02 -4.03962806e-02\n",
      "  4.66297846e-03  4.81598154e-02 -3.99202714e-03 -6.26806309e-03\n",
      "  3.87901291e-02  7.48795494e-02  3.37398946e-02  8.36968273e-02\n",
      "  1.91455502e-02 -1.53303713e-01 -1.27290890e-01 -6.90644141e-04\n",
      " -3.74577716e-02  3.89807150e-02 -1.22431837e-01 -8.49250406e-02\n",
      " -6.91415220e-02 -1.12124663e-02 -5.68320416e-02 -2.51073670e-02\n",
      "  6.42556399e-02  4.97863302e-03  8.83581210e-03 -3.12068593e-02\n",
      "  1.11401469e-01  4.44644969e-03 -1.58244491e-01 -4.44926471e-02\n",
      " -1.50468796e-01  1.29379183e-01  6.65319562e-02 -1.64371043e-01\n",
      " -7.65418187e-02 -2.78274305e-02  3.62538546e-02 -1.62226837e-02\n",
      " -1.38656739e-02  4.12445748e-03  1.83542684e-01 -7.71097690e-02\n",
      " -2.69008949e-02 -6.17867671e-02 -2.52384618e-02 -1.10330336e-01\n",
      " -4.71430309e-02  1.58317648e-02  4.75096405e-02 -1.18287876e-02\n",
      " -2.71452636e-01 -4.83341962e-02 -3.16344723e-02 -1.53666198e-01\n",
      "  2.21983925e-01  7.79224932e-02  2.92392522e-02 -4.71379273e-02\n",
      " -3.73414606e-02  7.70064071e-03 -7.58475289e-02 -5.70445694e-02\n",
      " -7.46037066e-02 -2.90890951e-02 -4.06749286e-02  7.11453408e-02\n",
      " -8.66067887e-04  1.53106423e-02 -1.08840764e-01  8.92872736e-02\n",
      "  1.02716975e-01 -8.55978653e-02 -9.36973840e-02  1.45265922e-01\n",
      "  2.65228748e-02 -1.32619724e-01 -2.17028521e-02 -6.07443079e-02\n",
      " -7.61815980e-02  8.14495236e-02 -4.47451659e-02 -5.81055507e-02\n",
      "  5.99262305e-02  6.66704625e-02 -1.66324656e-02 -4.17803787e-03\n",
      " -2.14972347e-01  1.98330302e-02 -1.70542412e-02 -1.06090233e-01\n",
      "  5.01890071e-02  4.83385623e-02  3.81411836e-02  1.58816114e-01\n",
      "  6.96531832e-02  2.06179600e-02  1.10727819e-02 -1.43180527e-02\n",
      " -1.07031025e-01 -8.64324495e-02  2.98522376e-02 -3.66399214e-02\n",
      "  4.01826017e-02  6.40841648e-02 -3.97763364e-02  1.56452674e-02\n",
      "  2.47866847e-02  1.86784104e-01  4.83903661e-02 -3.27205919e-02\n",
      " -8.21374357e-02 -2.20940895e-02 -1.18485734e-01 -8.17782581e-02\n",
      "  8.11422691e-02 -1.56671345e-01  8.37983936e-02  1.33957535e-01\n",
      " -7.02489242e-02  4.41226065e-02 -1.87253729e-01  5.69893494e-02\n",
      " -8.46961588e-02  2.13468224e-01 -4.77714390e-02 -1.15711883e-01\n",
      " -7.59830279e-03  3.61373499e-02  9.86962169e-02  1.53354213e-01\n",
      " -3.81403454e-02 -2.53000140e-01 -6.77316636e-02  5.77817373e-02\n",
      "  7.24089611e-03  2.68829819e-02 -4.94030192e-02 -1.90456379e-02\n",
      "  6.00438267e-02 -3.11915502e-02 -1.18714623e-01  2.79093701e-02\n",
      " -2.74217054e-02  1.35876462e-01  1.37320012e-01 -6.97149560e-02\n",
      "  3.41874585e-02 -5.91411218e-02 -3.78507338e-02 -1.77372918e-01\n",
      "  8.29180181e-02  3.13766785e-02 -2.71803588e-02 -3.62439156e-02\n",
      "  2.47425780e-01  3.55433226e-02  2.43287291e-02 -3.66697796e-02\n",
      " -2.38165602e-01  9.10897646e-03  2.41518766e-02 -7.07640350e-02\n",
      " -2.99290530e-02  1.30244091e-01  6.90960139e-02  3.72490510e-02\n",
      " -9.69134867e-02  1.62820257e-02  5.40136136e-02 -1.44067377e-01\n",
      " -1.63696893e-02 -1.50466412e-01  1.36355355e-01  1.04911961e-02\n",
      "  1.06531918e-01 -2.56005198e-01  8.39661211e-02  9.16278809e-02\n",
      "  1.09265812e-01 -1.55448526e-01 -9.23084691e-02  8.36678073e-02\n",
      " -1.06448621e-01  4.58171107e-02  4.05973829e-02 -2.19054222e-01\n",
      " -2.53252923e-01 -1.18067250e-01 -2.19414085e-02 -5.33010811e-02\n",
      " -1.75647028e-02  1.04526086e-02  6.55554831e-02  6.63069915e-03\n",
      " -1.81179997e-02 -1.80462390e-01 -1.52785733e-01 -3.77835184e-02\n",
      " -1.09250940e-01 -3.34075913e-02  1.74559429e-01 -1.97819602e-02\n",
      " -2.61032470e-02  5.51786721e-02  6.68683797e-02  3.77074108e-02\n",
      "  1.61187559e-01 -6.30677072e-03  2.55924851e-01 -2.81077214e-02\n",
      " -9.37069394e-03 -1.66612975e-02  6.95540980e-02  4.24879715e-02\n",
      " -4.70431633e-02 -4.71766442e-02  8.39391872e-02 -1.40381545e-01\n",
      " -6.59453124e-02 -9.90897324e-03 -1.33550659e-01  2.64872998e-01\n",
      "  1.47540867e-01  8.29333216e-02 -9.45618376e-02  3.48720104e-02\n",
      " -4.27380800e-02 -4.14666235e-02  2.93787062e-01  8.09783787e-02\n",
      "  5.44747859e-02 -8.36039335e-02 -5.05507551e-02  1.12362569e-02\n",
      "  1.23891002e-02  1.79864034e-01 -3.97053689e-01 -1.43569568e-02\n",
      " -4.19650227e-02  2.50575721e-01 -2.99447570e-02 -2.30750576e-01\n",
      " -7.36534549e-03  6.29367158e-02  5.20561710e-02 -1.12595692e-01\n",
      " -1.15811035e-01  7.08343312e-02  9.22225937e-02  9.87395197e-02\n",
      " -4.68517959e-01 -1.57914191e-01 -7.74041861e-02  1.67767271e-01\n",
      "  9.85915363e-02  1.59659117e-01  6.15486205e-02  3.92097384e-02\n",
      " -2.74677090e-02  4.70808387e-01  2.00689569e-01 -7.15709403e-02\n",
      "  2.83066053e-02  1.17469855e-01 -4.47368361e-02 -8.61830357e-03\n",
      " -2.56963950e-02  3.36213931e-02  5.88353500e-02  8.19693133e-02\n",
      " -2.30552815e-02 -8.03407729e-02  5.41035011e-02  4.23868001e-02\n",
      "  7.63841197e-02  9.82721448e-02 -2.25472283e-02 -9.10800919e-02\n",
      " -5.26146889e-02  9.29866824e-03  6.00511730e-02  7.38414377e-02\n",
      " -6.31466135e-02  4.75862995e-02  1.53863609e-01  3.50842737e-02\n",
      "  4.29953337e-02 -7.72519559e-02 -6.73987269e-02 -6.43547550e-02\n",
      " -9.96324271e-02  2.63704598e-01 -7.96668977e-02  1.93507567e-01\n",
      "  8.12023133e-02  3.52450721e-02  1.42432019e-01 -1.54847046e-02\n",
      "  1.21054482e-02  8.73979926e-03 -2.82382388e-02  1.09632716e-01\n",
      " -4.31104377e-02 -3.66651230e-02  6.47569448e-02 -2.47443374e-02\n",
      "  9.16703977e-03  1.05868965e-01  6.58208579e-02  3.49016637e-01\n",
      "  1.03876673e-01 -9.03906971e-02 -3.03679388e-02 -5.00512943e-02\n",
      "  1.47247044e-02  2.17527896e-02  4.47828136e-02 -1.25041073e-02\n",
      "  2.18731657e-01 -3.15186940e-02  4.48920354e-02 -9.06357169e-02\n",
      "  1.05849259e-01 -2.72975042e-02 -4.42415550e-02  1.38470501e-01\n",
      " -9.34329554e-02 -6.32722527e-02 -9.24995020e-02  6.66293036e-03\n",
      "  1.69599861e-01  3.32404003e-02 -3.70873101e-02  6.68590963e-02\n",
      "  1.65495854e-02  1.20882668e-01  1.22097716e-01  7.42692128e-02\n",
      "  3.76561098e-02 -1.51338711e-01 -2.71544028e-02  8.78081843e-02\n",
      " -1.12811513e-02  6.86467811e-02 -1.75218940e-01  1.08547479e-01\n",
      "  9.83297080e-02  9.24495161e-02 -5.65736853e-02 -9.20949802e-02\n",
      " -7.56276250e-02 -1.99819446e-01  1.01931259e-01  1.28271850e-02\n",
      "  4.31676060e-02 -2.26884350e-01  6.41151518e-02 -1.39952386e-02\n",
      "  9.04184356e-02 -4.12422977e-02 -6.59990609e-02 -5.42237330e-03\n",
      "  1.93747357e-02  1.41393009e-03  6.08156100e-02  9.68848988e-02\n",
      "  7.29130814e-03 -1.27508417e-01  5.77388927e-02  2.84573622e-02\n",
      "  2.66399458e-02  4.69285883e-02 -5.73017262e-02 -1.89315692e-01\n",
      "  3.18121649e-02  3.66618522e-02  4.43700179e-02 -4.87286970e-02\n",
      " -1.25282556e-01 -1.30572834e-03 -1.07260956e-03 -3.18001844e-02\n",
      "  6.99927360e-02  9.05432403e-02  1.58397369e-02  9.08737704e-02\n",
      "  6.22735843e-02  7.49477372e-02 -1.66865945e-01  9.49959531e-02\n",
      "  3.04888487e-02 -2.48598643e-02  7.18236789e-02  2.64322571e-02\n",
      " -6.42555878e-02  5.95403276e-02  3.19720507e-02 -5.00764623e-02\n",
      " -4.99993935e-02 -9.97407641e-03  2.56236605e-02  1.03109673e-01\n",
      " -7.22504631e-02  1.53109461e-01  1.29958957e-01 -4.60521737e-03\n",
      " -4.93538119e-02  1.10370882e-01  3.00523024e-02  1.27504230e-01\n",
      " -4.40188013e-02  3.01003102e-02  1.11278698e-01  3.94005105e-02\n",
      "  5.77408485e-02  1.03996269e-01  1.49964064e-01 -1.22475393e-01\n",
      "  9.58767813e-03  1.51009202e-01 -3.51378089e-03  9.89343897e-02\n",
      " -2.24322006e-02 -7.28162229e-02  2.84792393e-01 -7.87647292e-02\n",
      "  1.27250655e-02 -1.14098221e-01 -1.97764784e-01  5.91420904e-02\n",
      " -3.67527418e-02 -5.38736060e-02  2.28782613e-02 -5.09201661e-02\n",
      " -5.03553823e-03 -4.37604040e-01 -1.77169684e-02  9.75515097e-02\n",
      " -3.89110297e-02 -3.32234763e-02 -1.34486943e-01  4.70380858e-02\n",
      "  2.83126812e-02  1.07547352e-02 -1.79768505e-03 -2.69655347e-01\n",
      "  1.77864619e-02 -4.61897179e-02 -2.01262031e-02 -7.19725862e-02\n",
      " -1.62725866e-01 -1.83217615e-01 -7.16347247e-02 -7.25437608e-03\n",
      "  1.68792941e-02  7.74449145e-04  4.37077917e-02  1.04865402e-01\n",
      " -8.01181886e-03  7.92899821e-03 -2.88444962e-02 -1.79739565e-01\n",
      "  3.88644710e-02 -7.62089193e-02  9.11790058e-02 -1.98461443e-01\n",
      "  4.59525995e-02  9.63445157e-02  3.11273579e-02 -7.02128857e-02\n",
      " -1.44154131e-01  1.74900874e-01 -1.31867617e-01  8.74160081e-02\n",
      " -3.10179219e-02  1.06965020e-01  6.11868799e-02  2.02641219e-01\n",
      "  1.54150575e-01 -1.88198909e-01 -3.01534887e-02 -1.80481859e-02\n",
      " -2.74596326e-02 -1.29177064e-01  1.79394558e-01  5.53991832e-02\n",
      " -1.44756250e-02  3.92752513e-02 -7.19482154e-02 -1.27713546e-01\n",
      " -1.31759852e-01  4.34354320e-02 -3.36419605e-03  8.37995410e-02\n",
      "  4.32606786e-02 -5.80360070e-02  8.28832611e-02 -1.25128925e-01\n",
      " -1.68831468e-01 -1.95953399e-01  3.47400010e-02 -2.14776788e-02\n",
      "  2.81082522e-02 -1.75275058e-01  6.19253255e-02  5.73543422e-02\n",
      " -1.67949460e-02  5.66011891e-02 -4.31634039e-01  4.21188436e-02\n",
      " -6.17381297e-02 -4.68986901e-03  5.93686895e-03  2.38450810e-01\n",
      "  1.10104628e-01 -3.37336324e-02  2.55657360e-02  9.70306695e-02\n",
      " -1.47707956e-02 -4.36263084e-02  1.17597401e-01  3.95403653e-02\n",
      "  3.93677093e-02  1.10575929e-01  1.34534359e-01  3.47844996e-02\n",
      "  3.84443372e-01  1.32228032e-01  3.03350817e-02  7.29521886e-02\n",
      " -2.61208147e-01  4.26642187e-02 -6.37924671e-03  8.76365229e-02\n",
      " -1.18846621e-03  7.87737593e-02  2.56772060e-02 -5.79205379e-02\n",
      "  1.23391235e-02  2.26138346e-02  5.48587814e-02 -1.47127762e-01\n",
      " -5.75531200e-02 -9.66141671e-02 -2.38142267e-01 -4.31335755e-02\n",
      " -3.02807719e-01 -1.09063722e-02  1.22841984e-01 -8.61555263e-02\n",
      "  6.29301295e-02 -4.96453084e-02 -6.77350350e-03  1.81991328e-02\n",
      "  9.87301916e-02 -8.28537792e-02 -4.27705981e-02 -1.09553449e-01\n",
      "  2.91781612e-02  1.19377887e-02 -3.95353511e-02 -8.41891989e-02]:\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the pretrained model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Generate embeddings for each fragment\n",
    "embeddings = model.encode(fragments)\n",
    "\n",
    "# Sample data\n",
    "print(\"Number of embeddings:\", len(embeddings))\n",
    "print(f\"\\nEmbedding for fragment 1:\\n {embeddings[0]}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable type of embeddings: <class 'numpy.ndarray'>\n",
      "Dimensions of embeddings: (66, 768)\n"
     ]
    }
   ],
   "source": [
    "# Print embeddings list characteristics\n",
    "print(\"Variable type of embeddings:\", type(embeddings))\n",
    "print(\"Dimensions of embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row corresponds to a fragment and each column to a dimension in the embedding space. In other words, each row in this matrix represents the embedding for a particular fragment.\n",
    "\n",
    "- The pre-trained model is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which tools and approaches would help you generate them easily and high-level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best ways to generate this embeedings is to use pre-trained word embeddings models like the one used. In this case, one has to choose a model that trained for the specific case we need. In this case for semantic search. \n",
    "\n",
    "There are other frameworks like TensorFlow or Pytorch that offer neural networks architectures for generating embeddings and one can design an specific model like Word2Vec or skip-gram. \n",
    "\n",
    "Another tool could be use the embeddings models of the OpenAI, which are avaiable using their API. The same for Google's Universal Sentence Encoder and Facebook's infersent which offer APIs to also generate embeddings in a very way.\n",
    "\n",
    "Finally a high-level approaches to generate embaddings when large datasets are used is the utilization of GPU in order to accelerate the training. In general, depending on the specifica case, we have to choose the best approach to generate embeddings in an efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. For every question, return a sorted list of the N pieces that relate the most to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(user_question, text_fragments, n=3):\n",
    "    \"\"\"\n",
    "    Get the most related text fragments to a user question.\n",
    "\n",
    "    Args:\n",
    "        user_question (str): The question asked by the user.\n",
    "        text_fragments (list): List of text fragments to compare with the user question.\n",
    "        n (int, optional): Number of most related fragments to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "               - similarities: List of tuples (fragment, similarity_score) for all fragments.\n",
    "               - sorted_fragments: List of tuples (fragment, similarity_score) for the top N related fragments.\n",
    "    \"\"\"\n",
    "    # Instantiate the pretrained model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embedding for user question\n",
    "    embedding_question = model.encode(user_question)\n",
    "\n",
    "    # Initialize a list to store the similarity between the question and each fragment\n",
    "    similarities = []\n",
    "\n",
    "    # Generate embeddings for each text fragment and calculate cosine similarity\n",
    "    for fragment in text_fragments:\n",
    "        embedding_fragment = model.encode(fragment)\n",
    "        similarity = cosine_similarity([embedding_question], [embedding_fragment], dense_output=True)[0][0]\n",
    "        similarities.append((fragment, similarity))\n",
    "\n",
    "    # Sort text fragments based on their similarity with the user question\n",
    "    sorted_fragments = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return the top N most relevant fragments\n",
    "    return sorted_fragments[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function to find the most related fragments\n",
    "user_question = \"How does machine learning help businesses?\"\n",
    "\n",
    "questions = [\n",
    "    \"How does machine learning help businesses?\",\n",
    "    \"How does machine learning improve financial services?\",\n",
    "    \"What are the strengths of supervised learning?\",\n",
    "    \"How does unsupervised machine learning differ from supervised learning?\",\n",
    "]\n",
    "\n",
    "# Number of fragments\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Question: How does machine learning help businesses? =============\n",
      "\n",
      "Machine learning helps businesses by driving growth, unlocking new revenue streams, and solving challenging problems. Data is the critical driving force behind business decision-making but traditionally, companies have used data from various sources, like customer feedback, employees, and finance\n",
      "=> Similarity: 0.8451827168464661\n",
      "\n",
      "\n",
      "Letâs take a look at machine learning applications in some key industries:\n",
      "Machine learning can support predictive maintenance, quality control, and innovative research in the manufacturing sector. Machine learning technology also helps companies improve logistical solutions, including assets, supply chain, and inventory management\n",
      "=> Similarity: 0.7533918619155884\n",
      "\n",
      "============ Question: How does machine learning improve financial services? =============\n",
      "\n",
      "Machine learning helps businesses by driving growth, unlocking new revenue streams, and solving challenging problems. Data is the critical driving force behind business decision-making but traditionally, companies have used data from various sources, like customer feedback, employees, and finance\n",
      "=> Similarity: 0.7387895584106445\n",
      "\n",
      " Machine learning technology can allow investors to identify new opportunities by analyzing stock market movements, evaluating hedge funds, or calibrating financial portfolios. In addition, it can help identify high-risk loan clients and mitigate signs of fraud\n",
      "=> Similarity: 0.7372057437896729\n",
      "\n",
      "============ Question: What are the strengths of supervised learning? =============\n",
      " \n",
      "The strengths of supervised learning are simplicity and ease of design. It's useful when predicting a possible limited set of outcomes, dividing data into categories, or combining results from two other machine learning algorithms\n",
      "=> Similarity: 0.7856079339981079\n",
      "\n",
      " The technique relies on using a small amount of labeled data and a large amount of unlabeled data to train systems. First, the labeled data is used to train the machine-learning algorithm partially\n",
      "=> Similarity: 0.5056772232055664\n",
      "\n",
      "============ Question: How does unsupervised machine learning differ from supervised learning? =============\n",
      "\n",
      "Unsupervised learning algorithms train on unlabeled data. They scan through new data, trying to establish meaningful connections between the inputs and predetermined outputs\n",
      "=> Similarity: 0.6672196388244629\n",
      "\n",
      "\n",
      "Unsupervised learning is useful for pattern recognition, anomaly detection, and automatically grouping data into categories. As the training data does not require labeling, set up is easy\n",
      "=> Similarity: 0.6388202905654907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the list of questions\n",
    "for user_question in questions:\n",
    "    related_fragments = get_answers(user_question, fragments, n=n)\n",
    "\n",
    "    # Results\n",
    "    print(f\"============ Question: {user_question} =============\")\n",
    "    for fragment, similarity in related_fragments:\n",
    "        print(f\"{fragment}\\n=> Similarity: {similarity}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the above results, a good general relationship between questions and answers is observed. The first answer, the one with the highest number in the cosine similarity calculation, is much more appropriate and consistent with the question, while the answers with lower similarity measure are sentences that mention or bear some relation to the words in the question, but are not a correct answer to it.\n",
    "\n",
    "Making some attempts with other text documents and questions, it is observed that the larger the document the less accurate the answer can become. This occurs when the words contained in the questions are repeated throughout the text in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What do you think that could make these types of systems more robust in terms of semantics and functionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve robustness of vector embedding systems, we propose:\n",
    "\n",
    "- Train the embeddings on large and diverse datasets spanning a wide range of languages, domains, and topics to capture a broad spectrum of semantic relationships or, alternatively, augment the training data with various transformations to increase the robustness of the embeddings to variations in the input data.\n",
    "\n",
    "- Use transfer learning of pre-trained embeddings to strengthen learning and improve robustness against sparse data in target tasks or domains.\n",
    "\n",
    "- Apply regularization techniques that help preserve semantic relationships, such as orthogonality regularization or semantic similarity constraints during training.\n",
    "\n",
    "In addition, different models of lexico-syntactic patterns could be incorporated to extract semantic relations such as synonymy, hyponymy and hyperonymy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "- AWS. (Consulted feb 2024). What is Machine Learning? Retrieved from https://aws.amazon.com/machine-learning/\n",
    "\n",
    "- Distance Metrics in Vector Search. (Consulted feb 2024). Weaviate. Retrieved from https://weaviate.io/blog/distance-metrics-in-vector-search\n",
    "\n",
    "- Hugging Face. (Consulted feb 2024). Retrieved from https://huggingface.co/\n",
    "\n",
    "- Perone, C. S. (2014). Word Embeddings: Introduction [Slideshare slides]. Retrieved from https://www.slideshare.net/perone/word-embeddings-introduction\n",
    "\n",
    "- Scikit-learn. (Consulted feb 2024). Cosine Similarity. Retrieved from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "- Sentence Transformers - Multilingual Sentence Embeddings. Hugging Face. Retrieved from https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
    "\n",
    "- Alarcón, C. (2023). Curso de Embeddings y Bases de Datos Vectoriales para NLP [Curso en línea]. Platzi. https://platzi.com/cursos/embeddings-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
